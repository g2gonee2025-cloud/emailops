diff --git a/backend/src/cortex/intelligence/graph_discovery.py b/backend/src/cortex/intelligence/graph_discovery.py
new file mode 100644
index 00000000..4b511baa
--- /dev/null
+++ b/backend/src/cortex/intelligence/graph_discovery.py
@@ -0,0 +1,132 @@
+"""
+Standalone library for discovering graph schema from conversation data.
+"""
+
+import concurrent.futures
+import logging
+import random
+from collections import Counter
+from typing import List
+
+from cortex.db.models import Chunk
+from cortex.db.session import get_db_session
+from cortex.intelligence.graph import GraphExtractor
+from rich.console import Console
+from rich.table import Table
+from sqlalchemy import func, select
+
+logger = logging.getLogger(__name__)
+
+
+def discover_graph_schema(*, tenant_id: str, sample_size: int = 20) -> None:
+    """
+    Analyzes a sample of conversations to discover the graph schema.
+
+    Args:
+        tenant_id: The tenant ID to analyze.
+        sample_size: The number of conversations to sample.
+    """
+
+    console = Console()
+
+    def get_sample_texts(db_session, *, tenant_id: str, sample_size: int) -> List[str]:
+        """Fetch a random sample of conversation texts."""
+        console.print(
+            f"Fetching {sample_size} random conversations for tenant '{tenant_id}'..."
+        )
+        stmt = (
+            select(Chunk.conversation_id)
+            .where(Chunk.tenant_id == tenant_id, Chunk.chunk_type == "message_body")
+            .group_by(Chunk.conversation_id)
+            .order_by(func.random())
+            .limit(sample_size)
+        )
+        conv_ids = db_session.execute(stmt).scalars().all()
+
+        if not conv_ids:
+            console.print("[yellow]No conversations found for this tenant.[/yellow]")
+            return []
+
+        texts = []
+        for cid in conv_ids:
+            chunks_stmt = (
+                select(Chunk.text)
+                .where(
+                    Chunk.conversation_id == cid,
+                    Chunk.chunk_type == "message_body",
+                )
+                .order_by(Chunk.position)
+            )
+            chunk_texts = db_session.execute(chunks_stmt).scalars().all()
+            full_text = "\n".join(chunk_texts)
+            texts.append(full_text)
+        return texts
+
+    def analyze_schema(texts: List[str]):
+        """Analyze the schema of the sampled texts."""
+        extractor = GraphExtractor()
+        node_types = Counter()
+        relations = Counter()
+        entity_names = []
+
+        console.print(
+            f"Starting graph extraction on {len(texts)} texts with ThreadPool..."
+        )
+
+        def process_text(text: str) -> dict:
+            try:
+                G = extractor.extract_graph(text)
+                n_types = [
+                    data.get("type", "UNKNOWN") for _, data in G.nodes(data=True)
+                ]
+                rels = [
+                    data.get("relation", "UNKNOWN")
+                    for _, _, data in G.edges(data=True)
+                ]
+                names = list(G.nodes())
+                return {"types": n_types, "relations": rels, "names": names}
+            except Exception as e:
+                logger.error(f"Extraction failed: {e}")
+                return {"types": [], "relations": [], "names": []}
+
+        try:
+            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
+                results = list(executor.map(process_text, texts))
+        except Exception as e:
+            logger.critical(f"ThreadPoolExecutor failed: {e}")
+            return
+
+        for res in results:
+            node_types.update(res["types"])
+            relations.update(res["relations"])
+            entity_names.extend(res["names"])
+
+        console.print("\n[bold green]=== SCHEMA DISCOVERY REPORT ===[/bold green]")
+
+        # Display Node Types
+        table = Table(title="Top 20 Node Types")
+        table.add_column("Type", style="cyan")
+        table.add_column("Count", style="magenta")
+        for k, v in node_types.most_common(20):
+            table.add_row(k, str(v))
+        console.print(table)
+
+        # Display Relationships
+        table = Table(title="Top 20 Relationships")
+        table.add_column("Relation", style="cyan")
+        table.add_column("Count", style="magenta")
+        for k, v in relations.most_common(20):
+            table.add_row(k, str(v))
+        console.print(table)
+
+        # Display Sample Entities
+        table = Table(title="Random Sample of 10 Entities")
+        table.add_column("Entity Name", style="cyan")
+        for name in random.sample(entity_names, min(10, len(entity_names))):
+            table.add_row(name)
+        console.print(table)
+
+    with get_db_session() as session:
+        texts = get_sample_texts(session, tenant_id=tenant_id, sample_size=sample_size)
+        if texts:
+            analyze_schema(texts)
diff --git a/cli/src/cortex_cli/cmd_graph.py b/cli/src/cortex_cli/cmd_graph.py
new file mode 100644
index 00000000..229847af
--- /dev/null
+++ b/cli/src/cortex_cli/cmd_graph.py
@@ -0,0 +1,31 @@
+"""CLI for graph-related commands."""
+
+import typer
+from cortex.intelligence.graph_discovery import discover_graph_schema as discover_schema_logic
+
+app = typer.Typer(
+    name="graph",
+    help="Commands for interacting with the Knowledge Graph.",
+    no_args_is_help=True,
+)
+
+
+@app.command("discover-schema")
+def discover_schema(
+    tenant_id: str = typer.Option(
+        ...,
+        "--tenant-id",
+        "-t",
+        help="Tenant ID to search within.",
+    ),
+    sample_size: int = typer.Option(
+        20,
+        "--sample-size",
+        "-n",
+        help="Number of conversations to sample.",
+    ),
+) -> None:
+    """
+    Discover the graph schema from a sample of conversations.
+    """
+    discover_schema_logic(tenant_id=tenant_id, sample_size=sample_size)
diff --git a/cli/src/cortex_cli/main.py b/cli/src/cortex_cli/main.py
index e0d85de3..577ca18f 100644
--- a/cli/src/cortex_cli/main.py
+++ b/cli/src/cortex_cli/main.py
@@ -146,6 +146,7 @@ def model_dump(self) -> dict[str, Any]: ...
 DATA_COMMANDS = [
     ("db", "Database management (stats, migrate)"),
     ("embeddings", "Embedding management (stats, backfill)"),
+    ("graph", "Knowledge Graph commands (discover-schema)"),
     ("s3", "S3/Spaces storage (list, ingest)"),
     ("maintenance", "System maintenance (resolve-entities)"),
 ]
@@ -1318,13 +1319,53 @@ def main(args: list[str] | None = None) -> None:
     # Register plugin subcommand groups
     from cortex_cli.cmd_db import setup_db_parser
     from cortex_cli.cmd_embeddings import setup_embeddings_parser
+    from cortex_cli.cmd_graph import app as graph_app
     from cortex_cli.cmd_maintenance import setup_maintenance_parser
     from cortex_cli.cmd_s3 import setup_s3_parser
+    import typer
+    from typer.main import get_command_from_info
+    from typer.core import TyperGroup
+    from rich.console import Console
+
+    # A bit of a hack to integrate Typer apps with argparse
+    def setup_typer_command(subparsers, name, app, help_text=""):
+        parser = subparsers.add_parser(name, help=help_text, add_help=False)
+        command_info = typer.main.get_command_info(
+            app,
+            name=name,
+            pretty_exceptions_short=False,
+            pretty_exceptions_show_locals=False,
+            rich_markup_mode="rich",
+        )
+        command = get_command_from_info(command_info, pretty_exceptions_short=False, pretty_exceptions_show_locals=False, rich_markup_mode="rich")
+
+        def _run_typer(args):
+            try:
+                if isinstance(command, TyperGroup):
+                     command(args.typer_args, standalone_mode=False)
+                else:
+                    command(standalone_mode=False)
+
+            except typer.Exit as e:
+                if e.code != 0:
+                    console = Console()
+                    console.print(f"[bold red]Error:[/bold red] {e}")
+                # Do not exit process
+            except Exception as e:
+                console = Console()
+                console.print(f"[bold red]An unexpected error occurred:[/bold red] {e}")
+
+
+        parser.set_defaults(func=lambda args: _run_typer(args))
+        # This is a simple way to pass through args. A more robust solution might be needed.
+        parser.add_argument("typer_args", nargs="*")
+
 
     setup_db_parser(subparsers)
     setup_embeddings_parser(subparsers)
     setup_s3_parser(subparsers)
     setup_maintenance_parser(subparsers)
+    setup_typer_command(subparsers, "graph", graph_app, help_text="Knowledge Graph commands")
 
     # Parse arguments
     parsed_args = parser.parse_args(args)
diff --git a/scripts/discover_graph_schema.py b/scripts/discover_graph_schema.py
index e43f1b46..b3b6c39c 100644
--- a/scripts/discover_graph_schema.py
+++ b/scripts/discover_graph_schema.py
@@ -1,112 +1,52 @@
-import concurrent.futures
-import logging
-import random
-from collections import Counter
+#! /usr/bin/env python
+"""
+Discovers the graph schema from a sample of conversations.
 
-from dotenv import load_dotenv
-from sqlalchemy import func, select
+This script is a wrapper around the `cortex graph discover-schema` CLI command.
+"""
 
-load_dotenv(".env")
+import subprocess
+import sys
+from pathlib import Path
 
-from cortex.db.models import Chunk  # noqa: E402
-from cortex.db.session import get_db_session  # noqa: E402
-from cortex.intelligence.graph import GraphExtractor  # noqa: E402
 
-# Configure Logging
-logging.basicConfig(
-    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
-)
-logger = logging.getLogger("graph_discovery")
+def main():
+    """Run the `cortex graph discover-schema` command."""
+    # Construct the path to the CLI entrypoint
+    cli_path = (
+        Path(__file__).parent.parent / "cli/src/cortex_cli/main.py"
+    ).resolve()
 
-
-def get_sample_texts(sample_size: int = 100) -> list[str]:
-    """Fetch the largest 100 conversations (by text size) from the DB."""
-    logger.info(f"Fetching {sample_size} largest conversations...")
-    with get_db_session() as session:
-        # 1. Find IDs of largest conversations by summing chunk lengths
-        # Using a subquery/CTE approach or grouping
-        # 1 Switch to random sampling to avoid OOM/Timeout on massive files
-        stmt = (
-            select(Chunk.conversation_id)
-            .where(Chunk.chunk_type == "message_body")
-            .group_by(Chunk.conversation_id)
-            .order_by(func.random())
-            .limit(20)  # Reduced to 20 for speed/stability
+    if not cli_path.exists():
+        print(
+            f"Error: Could not find CLI entrypoint at {cli_path}", file=sys.stderr
         )
-        conv_ids = session.execute(stmt).scalars().all()
-
-        texts = []
-        for cid in conv_ids:
-            # 2. Reconstruct full text for each conversation (or at least the first 20k chars)
-            chunks_stmt = (
-                select(Chunk.text)
-                .where(Chunk.conversation_id == cid, Chunk.chunk_type == "message_body")
-                .order_by(Chunk.position)
-            )
-            chunk_texts = session.execute(chunks_stmt).scalars().all()
-            full_text = "\n".join(chunk_texts)
-            texts.append(full_text)
-
-        return texts
-
+        sys.exit(1)
 
-def analyze_schema(texts: list[str]):
-    extractor = GraphExtractor()
+    # Forward all arguments to the CLI
+    command = [
+        sys.executable,
+        str(cli_path),
+        "graph",
+        "discover-schema",
+        *sys.argv[1:],
+    ]
 
-    node_types = Counter()
-    relations = Counter()
-    entity_names = []
-
-    logger.info(f"Starting graph extraction on {len(texts)} texts with ThreadPool...")
-
-    def process_text(text: str) -> dict:
-        try:
-            G = extractor.extract_graph(text)
-            n_types = [data.get("type", "UNKNOWN") for _, data in G.nodes(data=True)]
-            rels = [
-                data.get("relation", "UNKNOWN") for _, _, data in G.edges(data=True)
-            ]
-            names = list(G.nodes())
-            return {"types": n_types, "relations": rels, "names": names}
-        except Exception as e:
-            logger.error(f"Extraction failed: {e}")
-            return {"types": [], "relations": [], "names": []}
-
-    # Parallelize LLM calls
     try:
-        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
-            results = list(executor.map(process_text, texts))
-    except Exception as e:
-        logger.critical(f"ThreadPoolExecutor failed: {e}")
-        return node_types, relations
-
-    # Aggregate results
-    for res in results:
-        node_types.update(res["types"])
-        relations.update(res["relations"])
-        entity_names.extend(res["names"])
-
-    logger.info("\n=== SCHEMA DISCOVERY REPORT ===")
-
-    logger.info("\nTOP NODE TYPES:")
-    for k, v in node_types.most_common(20):
-        logger.info(f"  {k}: {v}")
-
-    logger.info("\nTOP RELATIONSHIPS:")
-    for k, v in relations.most_common(20):
-        logger.info(f"  {k}: {v}")
-
-    # Heuristic check for "Missed" standard types or "Hallucinated" weird ones
-    logger.info("\nSample Entities (Random 10):")
-    for name in random.sample(entity_names, min(10, len(entity_names))):
-        logger.info(f"  - {name}")
-
-    return node_types, relations
+        subprocess.run(command, check=True)
+    except FileNotFoundError:
+        print(
+            f"Error: '{sys.executable}' not found.",
+            file=sys.stderr,
+        )
+        sys.exit(1)
+    except subprocess.CalledProcessError as e:
+        print(
+            f"Error executing command: {' '.join(command)}",
+            file=sys.stderr,
+        )
+        sys.exit(e.returncode)
 
 
 if __name__ == "__main__":
-    texts = get_sample_texts(100)
-    if texts:
-        analyze_schema(texts)
-    else:
-        logger.error("No texts found in DB.")
+    main()
