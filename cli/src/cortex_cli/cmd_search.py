
from __future__ import annotations

import contextlib
import hashlib
import html
import json
import logging
import mimetypes
import os
import re
import threading
import time
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timezone
from email.message import EmailMessage
from email.utils import formatdate, make_msgid, parseaddr
from pathlib import Path
from typing import Any, Generic, TypeVar

import numpy as np
import typer
from rich import print
from rich.table import Table

from cortex.config.loader import EmailOpsConfig
from ._config_helpers import resolve_index_dir, resolve_sender

# Assuming these are or will be in the cortex_cli package
# Fallback stubs will be created if they don't exist
try:
    from backend.src.cortex.llm.client import (
        LLMError,
        complete_json,
        complete_text,
        embed_texts,
    )
except ImportError:
    print(
        "[bold yellow]Warning:[/bold yellow] Could not import backend LLM modules. Using dummy functions."
    )
    T = TypeVar("T")
    def _dummy_embed(texts: list[str], provider: str = "") -> np.ndarray:
        print(f"DUMMY: Embedding {len(texts)} texts with {provider}")
        return np.random.rand(len(texts), 768).astype("float32")

    def _dummy_complete_text(
        system: str, user: str, max_output_tokens: int, temperature: float, stop_sequences: list[str]
    ) -> str:
        print("DUMMY: Completing text")
        return "This is a dummy response."

    def _dummy_complete_json(
        system: str, user: str, max_output_tokens: int, temperature: float, response_schema: dict
    ) -> str:
        print("DUMMY: Completing JSON")
        return json.dumps({"email_draft": "Dummy email draft.", "citations": []})

    LLMError = type("LLMError", (Exception,), {})
    embed_texts = _dummy_embed
    complete_text = _dummy_complete_text
    complete_json = _dummy_complete_json


try:
    from backend.src.cortex.indexing.metadata import (
        INDEX_DIRNAME_DEFAULT,
        load_index_metadata,
        read_mapping,
        safe_load_array,
        validate_index_compatibility,
    )
    from backend.src.cortex.processing.util_processing import (
        clean_email_text,
        should_skip_retrieval_cleaning,
    )
    from backend.src.cortex.util.files import read_text_file
    from backend.src.cortex.util.validators import (
        Result,
        validate_file_result,
    )
except ImportError:
    print(
        "[bold yellow]Warning:[/bold yellow] Could not import backend indexing/processing modules."
    )
    # Dummy fallbacks for core functions to allow basic CLI to load
    INDEX_DIRNAME_DEFAULT = "_index"
    T = TypeVar("T")
    def read_mapping(path: Path) -> list:
        return []

    def load_index_metadata(path: Path) -> dict:
        return {}

    def validate_index_compatibility(*args, **kwargs) -> bool:
        return True

    def clean_email_text(text: str) -> str:
        return text

    def should_skip_retrieval_cleaning(item: dict) -> bool:
        return False

    def read_text_file(path: Path) -> str:
        return "dummy content"

    @dataclass
    class Result(Generic[T]):
        ok: bool
        value: T | None = None
        error: str = ""

        @classmethod
        def success(cls, value: T) -> Result[T]:
            return cls(ok=True, value=value, error="")

        @classmethod
        def failure(cls, error: str) -> Result[T]:
            return cls(ok=False, value=None, error=str(error or ""))

    def validate_file_result(
        path: Path, must_exist: bool = True, allow_parent_traversal: bool = False
    ) -> Result[Path]:
        if must_exist and not path.exists():
            return Result.failure("File does not exist")
        return Result.success(path)

    def safe_load_array(path, mmap_mode="r"):
        class DummyArray:
            def __enter__(self):
                return np.array([])
            def __exit__(self, exc_type, exc_val, exc_tb):
                pass
        return DummyArray()

logger = logging.getLogger(__name__)
# Python <3.11 compatibility: datetime.UTC
UTC = timezone.utc

# ---------------------------- Configuration ---------------------------- #

RUN_ID = os.getenv("RUN_ID") or uuid.uuid4().hex

# Load configuration
cfg = EmailOpsConfig.load()

# Set sender configuration with defaults for testing/development
SENDER_LOCKED_NAME = cfg.email.sender_locked_name or "Default Sender"
SENDER_LOCKED_EMAIL = cfg.email.sender_locked_email or "default@example.com"
SENDER_LOCKED = f"{SENDER_LOCKED_NAME} <{SENDER_LOCKED_EMAIL}>"

ALLOWED_SENDERS = {
    s.strip() for s in os.getenv("ALLOWED_SENDERS", "").split(",") if s.strip()
}
SENDER_REPLY_TO = os.getenv("SENDER_REPLY_TO", "").strip()


MESSAGE_ID_DOMAIN = cfg.email.message_id_domain or "example.com"
REPLY_POLICY_DEFAULT = os.getenv("REPLY_POLICY", "reply_all").strip().lower()

INDEX_DIRNAME = os.getenv("INDEX_DIRNAME", INDEX_DIRNAME_DEFAULT)
MAPPING_NAME = "mapping.json"

# conservative char budget â‰ˆ tokens * 4
CHARS_PER_TOKEN = float(os.getenv("CHARS_PER_TOKEN", "3.8"))

# snippet char limits
CONTEXT_SNIPPET_CHARS_DEFAULT = int(os.getenv("CONTEXT_SNIPPET_CHARS", "1600"))

# recency / candidate tuning
HALF_LIFE_DAYS = max(1, int(os.getenv("HALF_LIFE_DAYS", "30")))
RECENCY_BOOST_STRENGTH = float(os.getenv("RECENCY_BOOST_STRENGTH", "1.0"))
CANDIDATES_MULTIPLIER = max(1, int(os.getenv("CANDIDATES_MULTIPLIER", "3")))
FORCE_RENORM = os.getenv("FORCE_RENORM", "0") == "1"
MIN_AVG_SCORE = float(os.getenv("MIN_AVG_SCORE", "0.2"))

# thresholds and targets
SIM_THRESHOLD_DEFAULT = float(os.getenv("SIM_THRESHOLD_DEFAULT", "0.30"))
REPLY_TOKENS_TARGET_DEFAULT = int(os.getenv("REPLY_TOKENS_TARGET_DEFAULT", "20000"))
FRESH_TOKENS_TARGET_DEFAULT = int(os.getenv("FRESH_TOKENS_TARGET_DEFAULT", "10000"))
BOOSTED_SCORE_CUTOFF = float(os.getenv("BOOSTED_SCORE_CUTOFF", "0.30"))
ATTACH_MAX_MB = float(os.getenv("ATTACH_MAX_MB", "15"))
ALLOW_PROVIDER_OVERRIDE = os.getenv("ALLOW_PROVIDER_OVERRIDE", "0") == "1"
PERSONA_DEFAULT = os.getenv("PERSONA", "expert insurance CSR").strip()

# Retrieval knobs
RERANK_ALPHA = float(
    os.getenv("RERANK_ALPHA", "0.35")
)  # weight for summary re-rank vs boosted
MMR_LAMBDA = float(os.getenv("MMR_LAMBDA", "0.70"))  # relevance vs diversity

# Validate ranges
if not (0.0 <= RERANK_ALPHA <= 1.0):
    raise ValueError(f"RERANK_ALPHA must be between 0.0 and 1.0, got {RERANK_ALPHA}")
if not (0.0 <= MMR_LAMBDA <= 1.0):
    raise ValueError(f"MMR_LAMBDA must be between 0.0 and 1.0, got {MMR_LAMBDA}")
MMR_K_CAP = int(os.getenv("MMR_K_CAP", "250"))  # safety cap for mmr selection set


# chat session storage
SESSIONS_DIRNAME = "_chat_sessions"
MAX_HISTORY_HARD_CAP = 5  # per requirement

# Token limits for different LLM operations
DRAFT_MAX_TOKENS = 1000
CRITIC_MAX_TOKENS = 800
AUDITOR_MAX_TOKENS = 350
IMPROVE_MAX_TOKENS = 1000
CHAT_MAX_TOKENS = 700

# Context size limits (characters)
REPLY_PER_DOC_LIMIT = 500_000
REPLY_MIN_DOC_LIMIT = 100_000
FRESH_PER_DOC_LIMIT = 250_000
FRESH_MIN_DOC_LIMIT = 50_000

# Search and ranking thresholds
TOP_FALLBACK_RESULTS = 10
FRESH_FALLBACK_RESULTS = 50

EMAIL_PATTERN = re.compile(r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$")

# P0-7 FIX: Comprehensive prompt injection patterns (based on research + OWASP)
INJECTION_PATTERNS = [
    # Classic jailbreak attempts
    "ignore previous instruction", "disregard earlier instruction", "override these rules",
    "forget all previous", "disregard all prior", "new instructions:", "updated instructions:",
    # System prompt manipulation
    "system prompt:", "system:", "assistant:", "### instruction", "### system",
    # Identity confusion
    "you are chatgpt", "you are now", "act as", "pretend you are", "as an ai language model",
    "as a large language model",
    # Code execution attempts
    "run code:", "execute:", "eval(", "exec(", "import os", "import sys", "subprocess", "__import__",
    # Mode switching
    "developer mode", "jailbreak", "debug mode", "admin mode", "god mode", "dan mode",
    # Prompt leaking
    "show me your prompt", "what are your instructions", "reveal your system prompt",
    "print your instructions",
    # Context injection
    "{{", "${", "<!--", "<script", "javascript:",
    # Role confusion
    "user:", "human:", "assistant:",
    # Base64/encoding tricks
    "base64", "decode(", "atob(",
    # Instruction termination
    "stop output", "end instructions", "ignore above",
]

# Compiled regex patterns for performance
_INJECTION_PATTERN_RE = re.compile(
    "|".join(re.escape(p) for p in INJECTION_PATTERNS), re.IGNORECASE
)

# Centralized audit rubric (names normalized)
AUDIT_RUBRIC = {
    "balanced_communication": "Tone is professional, empathetic, and concise; correct formality.",
    "displays_excellence": "Structure, clarity, and polish suitable for client-facing emails.",
    "factuality_rating": "All facts derived from provided snippets; no fabrication.",
    "utility_maximizing_communication": "Maximizes helpfulness and next-step clarity for the recipient.",
    "citation_quality": "Citations present for facts and are appropriate in scope.",
}
AUDIT_TARGET_MIN_SCORE = int(os.getenv("AUDIT_TARGET_MIN_SCORE", "8"))
# ------------------------------ Utilities ------------------------------ #
def _parse_date_any(date_str: str | None) -> datetime | None:
    if not date_str:
        return None
    s = str(date_str).strip()
    try:
        dt = datetime.fromisoformat(s.replace("Z", "+00:00"))
        return dt if dt.tzinfo else dt.replace(tzinfo=UTC)
    except Exception:
        pass
    try:
        from email.utils import parsedate_to_datetime

        dt = parsedate_to_datetime(s)
        return dt if dt.tzinfo else dt.replace(tzinfo=UTC)
    except Exception:
        return None

def _ensure_embeddings_ready(
    ix_dir: Path, mapping: list[dict[str, Any]]
) -> np.ndarray | None:
    """
    Load embeddings with proper cleanup (memmap mode, but returns in-memory copy).
    """
    emb_path = ix_dir / "embeddings.npy"
    if not emb_path.exists():
        return None

    try:
        with safe_load_array(emb_path, mmap_mode="r") as embs:
            if embs.ndim != 2 or embs.shape[1] <= 0:
                return None

            # Validate count match
            if embs.shape[0] != len(mapping):
                n = min(embs.shape[0], len(mapping))
                # Copy to memory to close memmap
                embs_mem = embs[:n].astype("float32", copy=True)
            else:
                # Copy entire array to memory
                embs_mem = embs.astype("float32", copy=True)

            # Optional re-normalization
            if FORCE_RENORM:
                norms = np.linalg.norm(embs_mem, axis=1, keepdims=True) + 1e-12
                if not np.allclose(float(norms.mean()), 1.0, atol=0.05):
                    embs_mem = (embs_mem / norms).astype("float32")
            return embs_mem
    except Exception as e:
        logger.warning(
            "Failed to load embeddings from %s: %s (index may need rebuild)",
            emb_path, e
        )
        import gc
        gc.collect()
        return None

def _normalize_email_field(v: Any) -> str:
    if not v:
        return ""
    if isinstance(v, dict):
        for k in ("smtp", "email", "address"):
            if v.get(k):
                return str(v[k]).strip().lower()
        if v.get("name"):
            _, addr = parseaddr(str(v.get("name")))
            return addr.strip().lower()
        return ""
    _, addr = parseaddr(str(v))
    return addr.strip().lower()

def _window_text_around_query(
    text: str, query: str, window: int = 1000, max_chars: int = 1600
) -> str:
    if not text:
        return ""
    if not query:
        return text[:max_chars]
    query_lower = query.lower()
    text_lower = text.lower()
    tokens = sorted(
        [
            w
            for w in query_lower.replace("/", " ").replace("\\", " ").split()
            if len(w) >= 3
        ],
        key=len,
        reverse=True,
    )
    pos = -1
    for token in tokens:
        pos = text_lower.find(token)
        if pos >= 0:
            break
    if pos < 0:
        return text[:max_chars]
    start = max(0, pos - window)
    end = min(len(text), pos + window)
    return text[start:end][:max_chars]

def _bidirectional_expand_text(
    text: str, start_pos: int, end_pos: int, max_chars: int
) -> str:
    if not text or start_pos < 0 or end_pos > len(text) or start_pos >= end_pos:
        return text[:max_chars]
    center_len = end_pos - start_pos
    remaining_budget = max(0, max_chars - center_len)
    expand_left = remaining_budget // 2
    expand_right = remaining_budget - expand_left
    start = max(0, start_pos - expand_left)
    end = min(len(text), end_pos + expand_right)
    if start == 0 and start_pos > 0:
        end = min(len(text), end + (expand_left - start_pos + start))
    if end == len(text) and end_pos < len(text):
        start = max(0, start - (end_pos + expand_right - len(text)))
    return text[start:end]

def _hard_strip_injection(text: str) -> str:
    """Heuristic prompt injection scrubber over raw file text slices."""
    if not text:
        return ""
    out = []
    for line in text.splitlines():
        if _line_is_injectionish(line):
            continue
        out.append(line)
    return "\n".join(out)

def _line_is_injectionish(_l: str) -> bool:
    ll = _l.strip().lower()
    if not ll:
        return False
    if any(p in ll for p in INJECTION_PATTERNS):
        return True
    # Drop lines that look like commands/prompts
    return bool(
        ll.startswith(
            ("system:", "assistant:", "user:", "instruction:", "### instruction", "```")
        )
    )

# ------------------------------ Filters (typed spec + simple grammar) ------------------------------ #
@dataclass
class SearchFilters:
    conv_ids: set[str] | None = None
    from_emails: set[str] | None = None
    to_emails: set[str] | None = None
    cc_emails: set[str] | None = None
    subject_contains: list[str] | None = None
    has_attachment: bool | None = None
    types: set[str] | None = None  # {'pdf','docx',...}
    date_from: datetime | None = None
    date_to: datetime | None = None
    exclude_terms: list[str] | None = None

_FILTER_TOKEN_RE = re.compile(
    r'(?P<key>subject|from|to|cc|after|before|has|type):(?P<value>"[^"]+"|\S+)',
    re.IGNORECASE,
)

def parse_filter_grammar(raw_query: str) -> tuple[SearchFilters, str]:
    """
    Tiny parser: extracts fielded tokens from the query and returns
    (filters, cleaned_free_text_query)
    """
    f = SearchFilters()
    q = raw_query or ""
    tokens = list(_FILTER_TOKEN_RE.finditer(q))
    # Remove tokens from the query string
    cleaned = q
    for m in reversed(tokens):
        start, end = m.span()
        cleaned = cleaned[:start] + cleaned[end:]
    cleaned = " ".join(cleaned.split())
    # Exclusions by leading '-' not part of fielded tokens
    exclude_terms = [t[1:] for t in cleaned.split() if t.startswith("-") and len(t) > 1]
    if exclude_terms:
        f.exclude_terms = [t.lower() for t in exclude_terms]
        # remove them from cleaned
        cleaned = " ".join(t for t in cleaned.split() if not t.startswith("-"))

    for m in tokens:
        key = m.group("key").lower()
        val = m.group("value")
        if val.startswith('"') and val.endswith('"'):
            val = val[1:-1]
        val = val.strip()
        if not val:
            continue
        if key == "subject":
            f.subject_contains = (f.subject_contains or []) + [val.lower()]
        elif key == "from":
            f.from_emails = (f.from_emails or set()) | {val.lower()}
        elif key == "to":
            f.to_emails = (f.to_emails or set()) | {val.lower()}
        elif key == "cc":
            f.cc_emails = (f.cc_emails or set()) | {val.lower()}
        elif key == "after":
            f.date_from = _parse_date_any(val) or f.date_from
        elif key == "before":
            f.date_to = _parse_date_any(val) or f.date_to
        elif key == "has":
            if val.lower() in {"attachment", "attachments"}:
                f.has_attachment = True
            elif val.lower() in {"noattachment", "no-attachment", "none"}:
                f.has_attachment = False
        elif key == "type":
            exts = {e.strip().lower().lstrip(".") for e in val.split(",") if e.strip()}
            f.types = (f.types or set()) | exts
    return f, cleaned

def apply_filters(mapping: list[dict[str, Any]], f: SearchFilters | None) -> list[int]:
    if not f:
        return list(range(len(mapping)))
    idx: list[int] = []
    for i, m in enumerate(mapping):
        subj = (m.get("subject") or "").lower()
        date_raw = (
            m.get("date")
            or m.get("end_date")
            or m.get("start_date")
            or m.get("modified_time")
        )
        att = bool(m.get("attachment_name"))
        ext = (m.get("attachment_type") or "").lower().lstrip(".")
        from_email = _normalize_email_field(m.get("from_email") or m.get("from"))
        to_emails = [
            _normalize_email_field(t)
            for t in (m.get("to_emails") or m.get("to") or [])
            if t
        ]
        cc_emails = [
            _normalize_email_field(c)
            for c in (m.get("cc_emails") or m.get("cc") or [])
            if c
        ]

        # conv id filter if provided
        if f.conv_ids and (str(m.get("conv_id") or "") not in f.conv_ids):
            continue
        if f.has_attachment is True and not att:
            continue
        if f.has_attachment is False and att:
            continue
        if f.types and ext and (ext not in f.types):
            continue
        if f.subject_contains and not all(x in subj for x in f.subject_contains):
            continue
        if f.exclude_terms and any(x in subj for x in f.exclude_terms):
            continue
        if f.from_emails and (from_email not in f.from_emails):
            continue
        if f.to_emails and not any(reci in f.to_emails for reci in to_emails):
            continue
        if f.cc_emails and not any(reci in f.cc_emails for reci in cc_emails):
            continue
        # date window
        if f.date_from or f.date_to:
            dt = _parse_date_any(date_raw)
            if f.date_from and (not dt or dt < f.date_from):
                continue
            if f.date_to and (not dt or dt > f.date_to):
                continue
        idx.append(i)
    return idx
# ------------------------------ Search Logic ------------------------------ #

def search(
    ix_dir: Path,
    query: str,
    k: int = 10,
    provider: str = "vertex",
    filters: SearchFilters | None = None,
    mmr_lambda: float = MMR_LAMBDA,
    rerank_alpha: float = RERANK_ALPHA,
) -> list[dict[str, Any]]:
    # ... (implementation to be filled in)
    return []

# ------------------------------ CLI App ------------------------------ #

app = typer.Typer(
    name="search",
    help="Search, draft, and chat with the email index.",
    no_args_is_help=True,
)


@app.command(name="query", help="Run a search query against the index.")
def query_cmd(
    query: str = typer.Argument(..., help="Search query with optional filters like 'subject:foo'"),
    k: int = typer.Option(10, "--k", "-k", help="Number of results to return."),
    provider: str = typer.Option("vertex", help="LLM provider (only vertex is supported)."),
    root_dir: Path = typer.Option(
        None,
        "--root",
        "-r",
        help="Export root directory containing the index.",
        rich_help_panel="Paths",
    ),
    from_filter: str = typer.Option(None, "--from", help="Filter by sender email."),
    to_filter: str = typer.Option(None, "--to", help="Filter by recipient email."),
    subject_filter: str = typer.Option(None, "--subject", help="Filter by subject line."),
):
    ix_dir = resolve_index_dir(root_dir)
    if not ix_dir.exists():
        err_console.print(f"[bold red]Error:[/bold red] Index not found at '{ix_dir}'. Please run indexing first.")
        raise typer.Exit(1)

    cli_filters = SearchFilters()
    if from_filter:
        cli_filters.from_emails = {e.strip().lower() for e in from_filter.split(",")}
    if to_filter:
        cli_filters.to_emails = {e.strip().lower() for e in to_filter.split(",")}
    if subject_filter:
        cli_filters.subject_contains = [subject_filter.lower()]

    results = search(
        ix_dir=ix_dir,
        query=query,
        k=k,
        provider=provider,
        filters=cli_filters,
    )

    if not results:
        console.print("No results found.")
        return

    table = Table(title=f"Search Results for '{query}'")
    table.add_column("Score", style="magenta")
    table.add_column("ID", style="cyan")
    table.add_column("Subject", style="green")
    table.add_column("Snippet")

    for r in results:
        snippet = (r.get("text") or "").replace("\n", " ")[:200] + "..."
        table.add_row(
            f"{r.get('score', 0):.3f}",
            r.get("id", ""),
            r.get("subject", ""),
            snippet,
        )
    console.print(table)


@app.command(help="List conversations, newest first.")
def list_conversations(
    root_dir: Path = typer.Option(
        None,
        "--root",
        "-r",
        help="Export root directory containing the index.",
        rich_help_panel="Paths",
    ),
):
    ix_dir = resolve_index_dir(root_dir)
    if not ix_dir.exists():
        err_console.print(f"[bold red]Error:[/bold red] Index not found at '{ix_dir}'.")
        raise typer.Exit(1)

    mapping = read_mapping(ix_dir)
    if not mapping:
        console.print("No conversations found in index.")
        return

    by_conv: dict[str, dict[str, Any]] = {}
    for m in mapping:
        cid = str(m.get("conv_id") or "")
        if not cid:
            continue
        d = _parse_date_any(m.get("date")) or _parse_date_any(m.get("modified_time"))
        subj = str(m.get("subject") or "")
        if cid not in by_conv:
            by_conv[cid] = {"conv_id": cid, "subject": subj, "last_date": d, "count": 1}
        else:
            by_conv[cid]["count"] += 1
            if d and (not by_conv[cid]["last_date"] or d > by_conv[cid]["last_date"]):
                by_conv[cid]["last_date"] = d
            if subj and not by_conv[cid]["subject"]:
                by_conv[cid]["subject"] = subj

    convs = sorted(list(by_conv.values()), key=lambda r: (r["last_date"] or datetime(1970, 1, 1, tzinfo=UTC)), reverse=True)

    for c in convs:
        date_str = c['last_date'].strftime('%Y-%m-%d %H:%M') if c['last_date'] else 'N/A'
        print(f"[cyan]{c['conv_id']}[/cyan] ({c['count']} items) - [bold]{c['subject']}[/bold] (last activity: {date_str})")


if __name__ == "__main__":
    app()
